# -*- coding: utf-8 -*-
"""Bigdl-based.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Px33-sFue2Fx00EMyc7616ZzDEN3vzhe

# Classification X-ray pneumonia images using Bigdl Approach

## Install Bigdl
"""

# uninstall pyspark, we install it with bigdl because of compatibility
!pip uninstall pyspark

pip install bigdl-orca-spark3

"""## Start SparkContext"""

from bigdl.orca import init_orca_context
init_orca_context(cluster_mode="local", memory="8g")

"""## Import Packages and Modules"""

import os
import shutil
import zipfile

import tensorflow as tf
from tensorflow import keras

"""## Download and Extract Dataset"""

def downlaod_extract_dataset():
  """
  This function downloads and extracts x-ray images from kaggle
  """

  # Create the .kaggle directory if it doesn't exist
  os.makedirs("/root/.kaggle", exist_ok=True)

  # Move the kaggle.json file
  shutil.move("kaggle.json", "/root/.kaggle/")

  # Set permissions
  os.chmod("/root/.kaggle/kaggle.json", 600)

  # Download kaggle dataset
  !kaggle datasets download -d paultimothymooney/chest-xray-pneumonia

  # Unzip the dataset
  with zipfile.ZipFile("/content/chest-xray-pneumonia.zip", 'r') as zip_ref:
    zip_ref.extractall("dataset")

  print("Dataset extracted successfully!")

downlaod_extract_dataset()

"""## Split dataset into Train, Test, Val"""

# Directories for train,val, test data
train_dir = "/content/dataset/chest_xray/train/"
val_dir = "/content/dataset/chest_xray/val/"
test_dir = "/content/dataset/chest_xray/test/"

BATCH_SIZE = 32
IMG_SIZE = 224
CHANNEL = 1


train_data = keras.preprocessing.image_dataset_from_directory(
    directory = train_dir,
    image_size = (IMG_SIZE,IMG_SIZE),
    label_mode = 'binary',
    batch_size = BATCH_SIZE,
    shuffle = True
).cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)

test_data = keras.preprocessing.image_dataset_from_directory(
    directory = test_dir,
    image_size = (IMG_SIZE,IMG_SIZE),
    label_mode = 'binary',
    batch_size = BATCH_SIZE
).cache().prefetch(buffer_size=tf.data.AUTOTUNE)

val_data = keras.preprocessing.image_dataset_from_directory(
    directory = val_dir,
    image_size = (IMG_SIZE,IMG_SIZE),
    label_mode = 'binary',
    batch_size = BATCH_SIZE
)

# The number of classes
class_names = val_data.class_names
val_data = val_data.cache().prefetch(buffer_size=tf.data.AUTOTUNE)
print(f"Class Names: {class_names}")

"""## Define Model"""

# Resize and Scaling Images
data_augmentation = keras.Sequential([
  keras.layers.Resizing(height=224, width=224),
  keras.layers.Rescaling(1./255)
], name ="data_augmentation")



model = keras.Sequential(
    [data_augmentation,
     keras.layers.Conv2D(20, kernel_size=(5, 5), strides=(1, 1), activation='relu',
                         padding='valid'),
     keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid'),
     keras.layers.Conv2D(50, kernel_size=(5, 5), strides=(1, 1), activation='relu',
                         padding='valid'),
     keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid'),
     keras.layers.Flatten(),
     keras.layers.Dense(500, activation='relu'),
     keras.layers.Dense(1, activation='sigmoid'),
     ]
)

model.compile(optimizer=keras.optimizers.RMSprop(),
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

"""## Orca Estimator"""

from bigdl.orca.learn.tf2.estimator import Estimator

!pip install ray[default]

est = Estimator.from_keras(keras_model=model)

"""## Fit Estimator on train data"""

est.fit(data=train_data,
        batch_size=BATCH_SIZE,
        epochs=30,
        validation_data=val_data)

"""## Evalute the Estimator"""

result = est.evaluate(test_data)
print(f"The Performance of the Model is: {result}")

"""## Stop SparkContext"""

from bigdl.orca import stop_orca_context

stop_orca_context()

"""-----------------"""