{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Classification X-ray pneumonia images using Bigdl Approach"
      ],
      "metadata": {
        "id": "WgYdF5d4tU2G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Bigdl"
      ],
      "metadata": {
        "id": "adrDbfaytMzm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y_Pbj6ByZgkJ",
        "outputId": "0416a22c-70ae-483e-ee19-80d9b1c1a704"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: pyspark 3.5.5\n",
            "Uninstalling pyspark-3.5.5:\n",
            "  Would remove:\n",
            "    /usr/local/bin/beeline\n",
            "    /usr/local/bin/beeline.cmd\n",
            "    /usr/local/bin/docker-image-tool.sh\n",
            "    /usr/local/bin/find-spark-home\n",
            "    /usr/local/bin/find-spark-home.cmd\n",
            "    /usr/local/bin/find_spark_home.py\n",
            "    /usr/local/bin/load-spark-env.cmd\n",
            "    /usr/local/bin/load-spark-env.sh\n",
            "    /usr/local/bin/pyspark\n",
            "    /usr/local/bin/pyspark.cmd\n",
            "    /usr/local/bin/pyspark2.cmd\n",
            "    /usr/local/bin/run-example\n",
            "    /usr/local/bin/run-example.cmd\n",
            "    /usr/local/bin/spark-class\n",
            "    /usr/local/bin/spark-class.cmd\n",
            "    /usr/local/bin/spark-class2.cmd\n",
            "    /usr/local/bin/spark-connect-shell\n",
            "    /usr/local/bin/spark-shell\n",
            "    /usr/local/bin/spark-shell.cmd\n",
            "    /usr/local/bin/spark-shell2.cmd\n",
            "    /usr/local/bin/spark-sql\n",
            "    /usr/local/bin/spark-sql.cmd\n",
            "    /usr/local/bin/spark-sql2.cmd\n",
            "    /usr/local/bin/spark-submit\n",
            "    /usr/local/bin/spark-submit.cmd\n",
            "    /usr/local/bin/spark-submit2.cmd\n",
            "    /usr/local/bin/sparkR\n",
            "    /usr/local/bin/sparkR.cmd\n",
            "    /usr/local/bin/sparkR2.cmd\n",
            "    /usr/local/lib/python3.11/dist-packages/pyspark-3.5.5.dist-info/*\n",
            "    /usr/local/lib/python3.11/dist-packages/pyspark/*\n",
            "Proceed (Y/n)? Y\n",
            "  Successfully uninstalled pyspark-3.5.5\n"
          ]
        }
      ],
      "source": [
        "# uninstall pyspark, we install it with bigdl because of compatibility\n",
        "!pip uninstall pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install bigdl-orca-spark3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DeK_wgRkbdDp",
        "outputId": "ef75994f-20d5-4cc9-c33e-d41f6a3c3591"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bigdl-orca-spark3\n",
            "  Downloading bigdl_orca_spark3-2.4.0-py3-none-manylinux1_x86_64.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from bigdl-orca-spark3) (24.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from bigdl-orca-spark3) (3.17.0)\n",
            "Collecting bigdl-tf==2.4.0.dev0 (from bigdl-orca-spark3)\n",
            "  Downloading bigdl_tf-2.4.0.dev0-py3-none-manylinux2010_x86_64.whl.metadata (299 bytes)\n",
            "Collecting bigdl-math==2.4.0.dev0 (from bigdl-orca-spark3)\n",
            "  Downloading bigdl_math-2.4.0.dev0-py3-none-manylinux2010_x86_64.whl.metadata (295 bytes)\n",
            "Collecting bigdl-dllib-spark3==2.4.0 (from bigdl-orca-spark3)\n",
            "  Downloading bigdl_dllib_spark3-2.4.0-py3-none-manylinux1_x86_64.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: pyzmq in /usr/local/lib/python3.11/dist-packages (from bigdl-orca-spark3) (24.0.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from bigdl-dllib-spark3==2.4.0->bigdl-orca-spark3) (2.0.2)\n",
            "Collecting pyspark==3.1.3 (from bigdl-dllib-spark3==2.4.0->bigdl-orca-spark3)\n",
            "  Downloading pyspark-3.1.3.tar.gz (214.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m214.0/214.0 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting conda-pack==0.3.1 (from bigdl-dllib-spark3==2.4.0->bigdl-orca-spark3)\n",
            "  Downloading conda_pack-0.3.1-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from bigdl-dllib-spark3==2.4.0->bigdl-orca-spark3) (1.17.0)\n",
            "Collecting bigdl-core==2.4.0.dev0 (from bigdl-dllib-spark3==2.4.0->bigdl-orca-spark3)\n",
            "  Downloading bigdl_core-2.4.0.dev0-py3-none-manylinux2010_x86_64.whl.metadata (291 bytes)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from conda-pack==0.3.1->bigdl-dllib-spark3==2.4.0->bigdl-orca-spark3) (75.1.0)\n",
            "Collecting py4j==0.10.9 (from pyspark==3.1.3->bigdl-dllib-spark3==2.4.0->bigdl-orca-spark3)\n",
            "  Downloading py4j-0.10.9-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Downloading bigdl_orca_spark3-2.4.0-py3-none-manylinux1_x86_64.whl (20.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.2/20.2 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bigdl_dllib_spark3-2.4.0-py3-none-manylinux1_x86_64.whl (64.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.0/64.0 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bigdl_math-2.4.0.dev0-py3-none-manylinux2010_x86_64.whl (35.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.4/35.4 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bigdl_tf-2.4.0.dev0-py3-none-manylinux2010_x86_64.whl (71.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.1/71.1 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bigdl_core-2.4.0.dev0-py3-none-manylinux2010_x86_64.whl (51.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.7/51.7 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading conda_pack-0.3.1-py2.py3-none-any.whl (27 kB)\n",
            "Downloading py4j-0.10.9-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m198.6/198.6 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.1.3-py2.py3-none-any.whl size=214463458 sha256=dbad31168072ca6fd2cc689b766c2728c62e6369d7d023f3af47ada0af8546b2\n",
            "  Stored in directory: /root/.cache/pip/wheels/25/da/89/3c1760252397d50554c2b3a66ab0ea57e1460fdab21d0aa968\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, bigdl-tf, bigdl-math, bigdl-core, pyspark, conda-pack, bigdl-dllib-spark3, bigdl-orca-spark3\n",
            "  Attempting uninstall: py4j\n",
            "    Found existing installation: py4j 0.10.9.7\n",
            "    Uninstalling py4j-0.10.9.7:\n",
            "      Successfully uninstalled py4j-0.10.9.7\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-spark-connect 0.5.2 requires pyspark>=3.5, but you have pyspark 3.1.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed bigdl-core-2.4.0.dev0 bigdl-dllib-spark3-2.4.0 bigdl-math-2.4.0.dev0 bigdl-orca-spark3-2.4.0 bigdl-tf-2.4.0.dev0 conda-pack-0.3.1 py4j-0.10.9 pyspark-3.1.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Start SparkContext"
      ],
      "metadata": {
        "id": "xPIt3UF68fXd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bigdl.orca import init_orca_context\n",
        "init_orca_context(cluster_mode=\"local\", memory=\"8g\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "id": "lalJVoHWtjhu",
        "outputId": "89f8cd56-5457-4453-98ed-d9750eeb834c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing orca context\n",
            "For Spark local mode, default to use all the cores on the node.\n",
            "Current pyspark location is : /usr/local/lib/python3.11/dist-packages/pyspark/__init__.py\n",
            "Start to getOrCreate SparkContext\n",
            "pyspark_submit_args is:  --driver-class-path /usr/local/lib/python3.11/dist-packages/bigdl/share/dllib/lib/bigdl-dllib-spark_3.1.3-2.4.0-jar-with-dependencies.jar:/usr/local/lib/python3.11/dist-packages/bigdl/share/core/lib/all-2.4.0-20230420.050641-1.jar:/usr/local/lib/python3.11/dist-packages/bigdl/share/orca/lib/bigdl-orca-spark_3.1.3-2.4.0-jar-with-dependencies.jar pyspark-shell \n",
            "Successfully got a SparkContext\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<SparkContext master=local[*] appName=pyspark-shell>"
            ],
            "text/html": [
              "\n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://c8179cc88c03:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.1.3</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Packages and Modules"
      ],
      "metadata": {
        "id": "XrZKPdcSsW-v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import zipfile\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n"
      ],
      "metadata": {
        "id": "wsOLFOiAkO6i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download and Extract Dataset"
      ],
      "metadata": {
        "id": "9jnBFKIOsSi8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def downlaod_extract_dataset():\n",
        "  \"\"\"\n",
        "  This function downloads and extracts x-ray images from kaggle\n",
        "  \"\"\"\n",
        "\n",
        "  # Create the .kaggle directory if it doesn't exist\n",
        "  os.makedirs(\"/root/.kaggle\", exist_ok=True)\n",
        "\n",
        "  # Move the kaggle.json file\n",
        "  shutil.move(\"kaggle.json\", \"/root/.kaggle/\")\n",
        "\n",
        "  # Set permissions\n",
        "  os.chmod(\"/root/.kaggle/kaggle.json\", 600)\n",
        "\n",
        "  # Download kaggle dataset\n",
        "  !kaggle datasets download -d paultimothymooney/chest-xray-pneumonia\n",
        "\n",
        "  # Unzip the dataset\n",
        "  with zipfile.ZipFile(\"/content/chest-xray-pneumonia.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"dataset\")\n",
        "\n",
        "  print(\"Dataset extracted successfully!\")\n"
      ],
      "metadata": {
        "id": "VwBniZ7OriZf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "downlaod_extract_dataset()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jo19fnvVki76",
        "outputId": "18b893d2-9e78-47f0-de20-563ad7355d8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.7.4.2 / client 1.6.17)\n",
            "Dataset URL: https://www.kaggle.com/datasets/paultimothymooney/chest-xray-pneumonia\n",
            "License(s): other\n",
            "Downloading chest-xray-pneumonia.zip to /content\n",
            "100% 2.29G/2.29G [00:21<00:00, 242MB/s]\n",
            "100% 2.29G/2.29G [00:21<00:00, 117MB/s]\n",
            "Dataset extracted successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split dataset into Train, Test, Val"
      ],
      "metadata": {
        "id": "62gE_2itsmlN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Directories for train,val, test data\n",
        "train_dir = \"/content/dataset/chest_xray/train/\"\n",
        "val_dir = \"/content/dataset/chest_xray/val/\"\n",
        "test_dir = \"/content/dataset/chest_xray/test/\""
      ],
      "metadata": {
        "id": "Ls1f2NxdkoQR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 32\n",
        "IMG_SIZE = 224\n",
        "CHANNEL = 1\n",
        "\n",
        "\n",
        "train_data = keras.preprocessing.image_dataset_from_directory(\n",
        "    directory = train_dir,\n",
        "    image_size = (IMG_SIZE,IMG_SIZE),\n",
        "    label_mode = 'binary',\n",
        "    batch_size = BATCH_SIZE,\n",
        "    shuffle = True\n",
        ").cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "test_data = keras.preprocessing.image_dataset_from_directory(\n",
        "    directory = test_dir,\n",
        "    image_size = (IMG_SIZE,IMG_SIZE),\n",
        "    label_mode = 'binary',\n",
        "    batch_size = BATCH_SIZE\n",
        ").cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "val_data = keras.preprocessing.image_dataset_from_directory(\n",
        "    directory = val_dir,\n",
        "    image_size = (IMG_SIZE,IMG_SIZE),\n",
        "    label_mode = 'binary',\n",
        "    batch_size = BATCH_SIZE\n",
        ")\n",
        "\n",
        "# The number of classes\n",
        "class_names = val_data.class_names\n",
        "val_data = val_data.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "print(f\"Class Names: {class_names}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mp4GbIH_s0g9",
        "outputId": "f66d0fdc-13eb-4586-d829-8e0d4f0129f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 5216 files belonging to 2 classes.\n",
            "Found 624 files belonging to 2 classes.\n",
            "Found 16 files belonging to 2 classes.\n",
            "Class Names: ['NORMAL', 'PNEUMONIA']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Model"
      ],
      "metadata": {
        "id": "8nwnIdb3t-PN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Resize and Scaling Images\n",
        "data_augmentation = keras.Sequential([\n",
        "  keras.layers.Resizing(height=224, width=224),\n",
        "  keras.layers.Rescaling(1./255)\n",
        "], name =\"data_augmentation\")\n",
        "\n",
        "\n",
        "\n",
        "model = keras.Sequential(\n",
        "    [data_augmentation,\n",
        "     keras.layers.Conv2D(20, kernel_size=(5, 5), strides=(1, 1), activation='relu',\n",
        "                         padding='valid'),\n",
        "     keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid'),\n",
        "     keras.layers.Conv2D(50, kernel_size=(5, 5), strides=(1, 1), activation='relu',\n",
        "                         padding='valid'),\n",
        "     keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid'),\n",
        "     keras.layers.Flatten(),\n",
        "     keras.layers.Dense(500, activation='relu'),\n",
        "     keras.layers.Dense(1, activation='sigmoid'),\n",
        "     ]\n",
        ")\n",
        "\n",
        "model.compile(optimizer=keras.optimizers.RMSprop(),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "oOAGTO8buAbZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Orca Estimator"
      ],
      "metadata": {
        "id": "I4xQxRGeu3fc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bigdl.orca.learn.tf2.estimator import Estimator"
      ],
      "metadata": {
        "id": "f-Hhqehaurjd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ray[default]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OGnrGTVhurfU",
        "outputId": "01828d39-50be-4ae8-c080-9dfc602b9d88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ray[default]\n",
            "  Downloading ray-2.43.0-cp311-cp311-manylinux2014_x86_64.whl.metadata (19 kB)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from ray[default]) (8.1.8)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from ray[default]) (3.17.0)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.11/dist-packages (from ray[default]) (4.23.0)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ray[default]) (1.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from ray[default]) (24.2)\n",
            "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /usr/local/lib/python3.11/dist-packages (from ray[default]) (4.25.6)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from ray[default]) (6.0.2)\n",
            "Requirement already satisfied: aiosignal in /usr/local/lib/python3.11/dist-packages (from ray[default]) (1.3.2)\n",
            "Requirement already satisfied: frozenlist in /usr/local/lib/python3.11/dist-packages (from ray[default]) (1.5.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from ray[default]) (2.32.3)\n",
            "Requirement already satisfied: aiohttp>=3.7 in /usr/local/lib/python3.11/dist-packages (from ray[default]) (3.11.13)\n",
            "Collecting aiohttp_cors (from ray[default])\n",
            "  Downloading aiohttp_cors-0.8.0-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting colorful (from ray[default])\n",
            "  Downloading colorful-0.5.6-py2.py3-none-any.whl.metadata (16 kB)\n",
            "Collecting py-spy>=0.2.0 (from ray[default])\n",
            "  Downloading py_spy-0.4.0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (16 kB)\n",
            "Requirement already satisfied: grpcio>=1.42.0 in /usr/local/lib/python3.11/dist-packages (from ray[default]) (1.71.0)\n",
            "Collecting opencensus (from ray[default])\n",
            "  Downloading opencensus-0.11.4-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: pydantic!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<3 in /usr/local/lib/python3.11/dist-packages (from ray[default]) (2.10.6)\n",
            "Requirement already satisfied: prometheus_client>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from ray[default]) (0.21.1)\n",
            "Requirement already satisfied: smart_open in /usr/local/lib/python3.11/dist-packages (from ray[default]) (7.1.0)\n",
            "Collecting virtualenv!=20.21.1,>=20.0.24 (from ray[default])\n",
            "  Downloading virtualenv-20.29.3-py3-none-any.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.7->ray[default]) (2.6.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.7->ray[default]) (25.3.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.7->ray[default]) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.7->ray[default]) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.7->ray[default]) (1.18.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<3->ray[default]) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<3->ray[default]) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<3->ray[default]) (4.12.2)\n",
            "Collecting distlib<1,>=0.3.7 (from virtualenv!=20.21.1,>=20.0.24->ray[default])\n",
            "  Downloading distlib-0.3.9-py2.py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: platformdirs<5,>=3.9.1 in /usr/local/lib/python3.11/dist-packages (from virtualenv!=20.21.1,>=20.0.24->ray[default]) (4.3.6)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray[default]) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray[default]) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray[default]) (0.23.1)\n",
            "Collecting opencensus-context>=0.1.3 (from opencensus->ray[default])\n",
            "  Downloading opencensus_context-0.1.3-py2.py3-none-any.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: six~=1.16 in /usr/local/lib/python3.11/dist-packages (from opencensus->ray[default]) (1.17.0)\n",
            "Requirement already satisfied: google-api-core<3.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from opencensus->ray[default]) (2.24.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->ray[default]) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->ray[default]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->ray[default]) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->ray[default]) (2025.1.31)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart_open->ray[default]) (1.17.2)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]) (1.69.1)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]) (1.26.1)\n",
            "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]) (2.38.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]) (4.9)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]) (0.6.1)\n",
            "Downloading py_spy-0.4.0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (2.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m47.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading virtualenv-20.29.3-py3-none-any.whl (4.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m99.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiohttp_cors-0.8.0-py3-none-any.whl (25 kB)\n",
            "Downloading colorful-0.5.6-py2.py3-none-any.whl (201 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.4/201.4 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opencensus-0.11.4-py2.py3-none-any.whl (128 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.2/128.2 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ray-2.43.0-cp311-cp311-manylinux2014_x86_64.whl (67.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.7/67.7 MB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading distlib-0.3.9-py2.py3-none-any.whl (468 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.0/469.0 kB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opencensus_context-0.1.3-py2.py3-none-any.whl (5.1 kB)\n",
            "Installing collected packages: py-spy, opencensus-context, distlib, colorful, virtualenv, aiohttp_cors, ray, opencensus\n",
            "Successfully installed aiohttp_cors-0.8.0 colorful-0.5.6 distlib-0.3.9 opencensus-0.11.4 opencensus-context-0.1.3 py-spy-0.4.0 ray-2.43.0 virtualenv-20.29.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "est = Estimator.from_keras(keras_model=model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wsKzyGL3urc2",
        "outputId": "1cbd6567-2fca-4740-87e5-4ff575884846"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:bigdl.orca.learn.tf2.ray_estimator:Please use load function of the estimator to load model when model_creator is None.\n",
            "2025-03-19 10:16:08,040\tINFO worker.py:1832 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://172.28.0.12:8265 \u001b[39m\u001b[22m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RayContext(dashboard_url='172.28.0.12:8265', python_version='3.11.11', ray_version='2.43.0', ray_commit='ecdcdc6a6e63dc4bcd6ea16aae256ce4d32a7e2c')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/ray/_private/worker.py:1241: UserWarning: len(ctx) is deprecated. Use len(ctx.address_info) instead.\n",
            "  warnings.warn(\"len(ctx) is deprecated. Use len(ctx.address_info) instead.\")\n",
            "\u001b[36m(Worker pid=2139)\u001b[0m 2025-03-19 10:16:14.488769: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "\u001b[36m(Worker pid=2139)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "\u001b[36m(Worker pid=2139)\u001b[0m E0000 00:00:1742379374.525150    2139 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "\u001b[36m(Worker pid=2139)\u001b[0m E0000 00:00:1742379374.538242    2139 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "\u001b[36m(Worker pid=2139)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.11/dist-packages/bigdl/orca/learn/tf2/tf_runner.py:337: _CollectiveAllReduceStrategyExperimental.__init__ (from tensorflow.python.distribute.collective_all_reduce_strategy) is deprecated and will be removed in a future version.\n",
            "\u001b[36m(Worker pid=2139)\u001b[0m Instructions for updating:\n",
            "\u001b[36m(Worker pid=2139)\u001b[0m use distribute.MultiWorkerMirroredStrategy instead\n",
            "\u001b[36m(Worker pid=2139)\u001b[0m 2025-03-19 10:16:18.982909: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "\u001b[36m(Worker pid=2139)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "\u001b[36m(Worker pid=2139)\u001b[0m I0000 00:00:1742379378.995401    2139 grpc_server_lib.cc:464] Started server with target: grpc://172.28.0.12:52577\n",
            "\u001b[36m(Worker pid=2139)\u001b[0m I0000 00:00:1742379379.014144    2139 coordination_service_agent.cc:340] Coordination agent has successfully connected.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fit Estimator on train data"
      ],
      "metadata": {
        "id": "GxvMdSsm5_uH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "est.fit(data=train_data,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        epochs=30,\n",
        "        validation_data=val_data)"
      ],
      "metadata": {
        "id": "9x9vb2Dmz-8F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evalute the Estimator"
      ],
      "metadata": {
        "id": "PCHRhgWJ6L_Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = est.evaluate(test_data)\n",
        "print(f\"The Performance of the Model is: {result}\")"
      ],
      "metadata": {
        "id": "vNXYtp8r6Jrf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stop SparkContext"
      ],
      "metadata": {
        "id": "X5JCQtIn70j2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bigdl.orca import stop_orca_context"
      ],
      "metadata": {
        "id": "w1eD2MxI78MZ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_orca_context()"
      ],
      "metadata": {
        "id": "L9NRvC-q8E8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----------------"
      ],
      "metadata": {
        "id": "QLZUfQEF86XW"
      }
    }
  ]
}